{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  7000  Val set:  900  Test set:  100\n"
     ]
    }
   ],
   "source": [
    "# You will need to adjust BATCH_SIZE based on how much memory your GPU has and how big the resize_input_to values are. \n",
    "\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import helper\n",
    "from torch.utils import data\n",
    "from datasets import BerkeleyDataSet, BerkeleyDataTestSet\n",
    "\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "resize_input_to = [640,320]   #Mistake! should be 640x360 for half-size images\n",
    "data_root = 'data\\\\bdd100k';\n",
    "list_root = os.path.join(data_root, 'lists\\\\100k\\\\drivable');\n",
    "\n",
    "train_dataset = BerkeleyDataSet(data_root, os.path.join(list_root, 'train_images.txt'), resize_to=resize_input_to);\n",
    "val_dataset = BerkeleyDataSet(data_root, os.path.join(list_root, 'val_images.txt'), resize_to=resize_input_to, train=False);\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=1, pin_memory=True)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=1)\n",
    "\n",
    "test_dataset = BerkeleyDataTestSet(data_root, os.path.join(list_root, 'test_images.txt'), resize_to=resize_input_to);\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "print(\"Train set: \", len(train_dataset), \" Val set: \", len(val_dataset), \" Test set: \", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "from loss import dice_loss\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    #bce = F.binary_cross_entropy_with_logits(pred, target)\n",
    "        \n",
    "    #pred = F.sigmoid(pred)\n",
    "    #dice = dice_loss(pred, target)\n",
    "    \n",
    "    #loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "    pred = pred.type(torch.FloatTensor)\n",
    "    target = target.type(torch.LongTensor)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=255)\n",
    "    loss = loss_fn(pred, target)\n",
    "    \n",
    "    #metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    #metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):    \n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "        \n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))    \n",
    "\n",
    "def train_model(model, optimizer, scheduler, epoch, best_loss, num_epochs=25):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('-' * 10)\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        \n",
    "        since = time.time()\n",
    "        loader = train_loader\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "                    \n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                loader = val_loader;\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "            \n",
    "            for i, batch in enumerate(loader):\n",
    "                scheduler.batch_step()\n",
    "                input_images, target_masks, _, _ = batch\n",
    "                \n",
    "                inputs = input_images.to(device)\n",
    "                labels = target_masks.to(device)             \n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = calc_loss(outputs, labels, metrics)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "            \n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(\"saving best model with loss: \", epoch_loss)\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "import torchsummary\n",
    "import unet_pytorch2\n",
    "from cyclic_lr import CyclicLR\n",
    "\n",
    "load_from_checkpoint = True\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_class = 19\n",
    "learn_rate = 1e-3\n",
    "\n",
    "if load_from_checkpoint:\n",
    "    model = unet_pytorch2.UNet(in_channels=3, n_classes=num_class, depth=5, wf=6, batch_norm=True, padding=True)\n",
    "    optimizer_ft = optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    checkpoint = torch.load('model/checkpoint.pt')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    optimizer_ft.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "else:\n",
    "    model = unet_pytorch2.UNet(in_channels=3, n_classes=num_class, depth=5, wf=6, batch_norm=True, padding=True).to(device)\n",
    "    optimizer_ft = optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    epoch = 0\n",
    "    loss = 1e10\n",
    "\n",
    "#torchsummary.summary(model, input_size=(3, resize_input_to[1], resize_input_to[0]), device='CUDA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Epoch 0/29\n",
      "LR 0.001\n",
      "train: loss: 2.370897\n",
      "val: loss: 2.292422\n",
      "saving best model with loss:  2.292422157128652\n",
      "44m 28s\n",
      "----------\n",
      "Epoch 1/29\n",
      "LR 0.002880714285714286\n",
      "train: loss: 2.269827\n",
      "val: loss: 2.233665\n",
      "saving best model with loss:  2.233664568265279\n",
      "44m 38s\n",
      "----------\n",
      "Epoch 2/29\n",
      "LR 0.004762142857142857\n",
      "train: loss: 2.240121\n",
      "val: loss: 2.222318\n",
      "saving best model with loss:  2.2223179880777995\n",
      "44m 6s\n",
      "----------\n",
      "Epoch 3/29\n",
      "LR 0.005356428571428571\n",
      "train: loss: 2.215293\n",
      "val: loss: 2.183039\n",
      "saving best model with loss:  2.1830392424265543\n",
      "43m 59s\n",
      "----------\n",
      "Epoch 4/29\n",
      "LR 0.0034750000000000007\n",
      "train: loss: 2.196058\n",
      "val: loss: 2.168759\n",
      "saving best model with loss:  2.1687585218747456\n",
      "43m 50s\n",
      "----------\n",
      "Epoch 5/29\n",
      "LR 0.001593571428571429\n",
      "train: loss: 2.185875\n",
      "val: loss: 2.166247\n",
      "saving best model with loss:  2.1662473400433857\n",
      "43m 29s\n",
      "----------\n",
      "Epoch 6/29\n",
      "LR 0.0022878571428571427\n",
      "train: loss: 2.188295\n",
      "val: loss: 2.165267\n",
      "saving best model with loss:  2.1652665837605793\n",
      "43m 41s\n",
      "----------\n",
      "Epoch 7/29\n",
      "LR 0.004169285714285715\n",
      "train: loss: 2.191958\n",
      "val: loss: 2.174654\n",
      "43m 39s\n",
      "----------\n",
      "Epoch 8/29\n",
      "LR 0.005949285714285714\n",
      "train: loss: 2.186504\n",
      "val: loss: 2.155942\n",
      "saving best model with loss:  2.1559418471654257\n",
      "43m 51s\n",
      "----------\n",
      "Epoch 9/29\n",
      "LR 0.004067857142857142\n",
      "train: loss: 2.176806\n",
      "val: loss: 2.154502\n",
      "saving best model with loss:  2.154502316315969\n",
      "43m 49s\n",
      "----------\n",
      "Epoch 10/29\n",
      "LR 0.0021864285714285705\n",
      "train: loss: 2.168977\n",
      "val: loss: 2.151753\n",
      "saving best model with loss:  2.1517530290285745\n",
      "43m 40s\n",
      "----------\n",
      "Epoch 11/29\n",
      "LR 0.0016950000000000012\n",
      "train: loss: 2.169728\n",
      "val: loss: 2.154593\n",
      "43m 36s\n",
      "----------\n",
      "Epoch 12/29\n",
      "LR 0.003576428571428573\n",
      "train: loss: 2.170807\n",
      "val: loss: 2.157932\n",
      "44m 31s\n",
      "----------\n",
      "Epoch 13/29\n",
      "LR 0.0054578571428571445\n",
      "train: loss: 2.170148\n",
      "val: loss: 2.141430\n",
      "saving best model with loss:  2.1414300123850505\n",
      "43m 47s\n",
      "----------\n",
      "Epoch 14/29\n",
      "LR 0.004660714285714284\n",
      "train: loss: 2.161298\n",
      "val: loss: 2.139670\n",
      "saving best model with loss:  2.1396695669492085\n",
      "43m 55s\n",
      "----------\n",
      "Epoch 15/29\n",
      "LR 0.0027792857142857124\n",
      "train: loss: 2.153175\n",
      "val: loss: 2.135229\n",
      "saving best model with loss:  2.135228593349457\n",
      "43m 28s\n",
      "----------\n",
      "Epoch 16/29\n",
      "LR 0.0011021428571428551\n",
      "train: loss: 2.150334\n",
      "val: loss: 2.141508\n",
      "43m 18s\n",
      "----------\n",
      "Epoch 17/29\n",
      "LR 0.002983571428571427\n",
      "train: loss: 2.155008\n",
      "val: loss: 2.136606\n",
      "44m 1s\n",
      "----------\n",
      "Epoch 18/29\n",
      "LR 0.004864999999999999\n",
      "train: loss: 2.158612\n",
      "val: loss: 2.144313\n",
      "43m 16s\n",
      "----------\n",
      "Epoch 19/29\n",
      "LR 0.00525357142857143\n",
      "train: loss: 2.152889\n",
      "val: loss: 2.130447\n",
      "saving best model with loss:  2.130447427431742\n",
      "43m 39s\n",
      "----------\n",
      "Epoch 20/29\n",
      "LR 0.0033721428571428583\n",
      "train: loss: 2.144384\n",
      "val: loss: 2.129682\n",
      "saving best model with loss:  2.1296818041801453\n",
      "43m 53s\n",
      "----------\n",
      "Epoch 21/29\n",
      "LR 0.0014907142857142866\n",
      "train: loss: 2.139463\n",
      "val: loss: 2.126899\n",
      "saving best model with loss:  2.1268994855880736\n",
      "43m 9s\n",
      "----------\n",
      "Epoch 22/29\n",
      "LR 0.002390714285714285\n",
      "train: loss: 2.143231\n",
      "val: loss: 2.136611\n",
      "43m 45s\n",
      "----------\n",
      "Epoch 23/29\n",
      "LR 0.004272142857142856\n",
      "train: loss: 2.146840\n",
      "val: loss: 2.144852\n",
      "43m 42s\n",
      "----------\n",
      "Epoch 24/29\n",
      "LR 0.005846428571428572\n",
      "train: loss: 2.145700\n",
      "val: loss: 2.129326\n",
      "43m 8s\n",
      "----------\n",
      "Epoch 25/29\n",
      "LR 0.003965\n",
      "train: loss: 2.139236\n",
      "val: loss: 2.124420\n",
      "saving best model with loss:  2.1244199156761168\n",
      "43m 55s\n",
      "----------\n",
      "Epoch 26/29\n",
      "LR 0.0020835714285714286\n",
      "train: loss: 2.133311\n",
      "val: loss: 2.121855\n",
      "saving best model with loss:  2.1218545794487\n",
      "43m 51s\n",
      "----------\n",
      "Epoch 27/29\n",
      "LR 0.0017978571428571436\n",
      "train: loss: 2.134071\n",
      "val: loss: 2.127987\n",
      "43m 28s\n",
      "----------\n",
      "Epoch 28/29\n",
      "LR 0.0036792857142857152\n",
      "train: loss: 2.139000\n",
      "val: loss: 2.132196\n",
      "44m 14s\n",
      "----------\n",
      "Epoch 29/29\n",
      "LR 0.005560714285714287\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-eb5f8e397fe0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcyclic_scheduler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCyclicLR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer_ft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcyclic_scheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-e4242876c5e2>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, scheduler, epoch, best_loss, num_epochs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                     \u001b[1;31m# backward + optimize only if in training phase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m                         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\dlautomotive\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\dlautomotive\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### TRAIN MODEL \n",
    "\n",
    "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=25, gamma=0.1)\n",
    "\n",
    "cyclic_scheduler = CyclicLR(optimizer_ft, step_size=7000)\n",
    "\n",
    "model = train_model(model, optimizer_ft, cyclic_scheduler, epoch, loss, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint model to disk\n",
    "\n",
    "torch.save({\n",
    "    'epoch' : epoch,\n",
    "    'model_state_dict' : model.state_dict(),\n",
    "    'optimizer_state_dict' : optimizer_ft.state_dict(),\n",
    "    'loss' : loss\n",
    "}, 'model/checkpoint.pt');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on test set\n",
    "# This block sets the model to evaluation mode and creates an iterator on the Test dataloader\n",
    "\n",
    "import math\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib\n",
    "from PIL import Image\n",
    "from sklearn.metrics import jaccard_similarity_score as jsc\n",
    "from torchnet.meter import APMeter\n",
    "\n",
    "NUM_TEST_SAMPLES = 30\n",
    "\n",
    "mean_ious = np.zeros(NUM_TEST_SAMPLES);\n",
    "ious_index = 0;\n",
    "\n",
    "map_scores = np.zeros(NUM_TEST_SAMPLES);\n",
    "\n",
    "apmeter = APMeter()\n",
    "\n",
    "results_folder = os.path.join(data_root, 'seg\\\\results\\\\final_jan25')\n",
    "gt_folder = os.path.join(data_root, 'seg\\\\results\\\\final_gt')\n",
    "model.eval()   # Set model to evaluate mode\n",
    "it = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n",
      "(320, 640)\n"
     ]
    }
   ],
   "source": [
    "# Each time this block is executed the next BATCH_SIZE test samples are loaded, evaluated and displayed. \n",
    "# The Groundtruth and Predicted labels are then written to 'data/bdd100k/seg/results/test' as .png images\n",
    "# You can execute this block over and over to work your way through the entire test set\n",
    "\n",
    "for b in range(100):\n",
    "\n",
    "    inputs, labels, filenames, _ = next(it)\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    pred = model(inputs)\n",
    "\n",
    "    pred = pred.data.cpu().numpy()\n",
    "    imgs = inputs.data.cpu().numpy()\n",
    "    labels = labels.data.cpu().numpy()\n",
    "\n",
    "    # Map each channel (i.e. class) to each color\n",
    "    target_masks_rgb = labels[0]\n",
    "    pred_rgb = np.argmax(pred[0], axis=0)\n",
    "    pred_rgb[target_masks_rgb == 255] = 255\n",
    "    print(pred_rgb.shape)\n",
    "    \n",
    "    cmap = plt.get_cmap('tab20')\n",
    "\n",
    "    predimg = Image.fromarray(pred_rgb.astype('uint8'))\n",
    "    predimg.save(os.path.join(results_folder, filenames[0]+'.png'))\n",
    "    \n",
    "    gtimg = Image.fromarray(target_masks_rgb.astype('uint8'))\n",
    "    gtimg.save(os.path.join(gt_folder, filenames[0]+'.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_segmentation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-a98770a2f1e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpredir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'seg\\\\results\\\\final_jan25'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mevaluate_segmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgtdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m19\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m17\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluate_segmentation' is not defined"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "gtdir = os.path.join(data_root, 'seg\\\\results\\\\final_gt')\n",
    "predir = os.path.join(data_root, 'seg\\\\results\\\\final_jan25')\n",
    "\n",
    "evaluate_segmentation(gtdir, predir, 19, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
